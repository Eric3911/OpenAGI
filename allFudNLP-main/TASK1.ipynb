{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务一：基于机器学习的文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步：数据获取与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train=pd.read_csv('E:/NLP_jupyternotebook/Fudan_NLP_beginner/data/train.tsv',header=0,delimiter='\\t')\n",
    "df_test=pd.read_csv('E:/NLP_jupyternotebook/Fudan_NLP_beginner/data/test.tsv',header=0,delimiter='\\t')\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.509945\n",
      "3    0.210989\n",
      "1    0.174760\n",
      "4    0.058990\n",
      "0    0.045316\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.Sentiment.value_counts()/df_train.Sentiment.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Phrase.str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由于只给了一个数据，要将数据集划分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_train['Phrase']\n",
    "y=df_train['Sentiment']\n",
    "ceshi_data=df_test['Phrase']\n",
    "all_data=list(X)\n",
    "all_lables=list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .',\n",
       " 'A series of escapades demonstrating the adage that what is good for the goose',\n",
       " 'A series',\n",
       " 'A',\n",
       " 'series',\n",
       " 'of escapades demonstrating the adage that what is good for the goose',\n",
       " 'of',\n",
       " 'escapades demonstrating the adage that what is good for the goose',\n",
       " 'escapades',\n",
       " 'demonstrating the adage that what is good for the goose',\n",
       " 'demonstrating the adage',\n",
       " 'demonstrating',\n",
       " 'the adage',\n",
       " 'the',\n",
       " 'adage',\n",
       " 'that what is good for the goose',\n",
       " 'that',\n",
       " 'what is good for the goose',\n",
       " 'what',\n",
       " 'is good for the goose']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词与词典：采用2-gram词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentiment(data):\n",
    "    def tokenizer(text):\n",
    "        return[tok.lower() for tok in text.split(' ')]\n",
    "    tokenized_data=[tokenizer(review) for review in data]\n",
    "    def ngram(tokenized_data):\n",
    "        data=[]\n",
    "        for text in tokenized_data:\n",
    "            if len(text)==1:\n",
    "                data.append(text)\n",
    "            else:\n",
    "                ng=[(a+' '+b)for a,b in zip(text[:-1],text[1:])]\n",
    "                data.append(ng)\n",
    "        return data\n",
    "    ngram_data=ngram(tokenized_data)\n",
    "    return ngram_data\n",
    "ngram_data=get_tokenized_sentiment(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典长度为:100664\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(ngram_data):\n",
    "    counter=collections.Counter([tk for st in ngram_data for tk in st])\n",
    "    idx_to_char=[item[0] for item in counter.items()]\n",
    "    char_to_idx=dict([(char,idx) for idx,char in enumerate(idx_to_char)])\n",
    "    return idx_to_char,char_to_idx\n",
    "idx_to_char,char_to_idx=get_vocab(ngram_data)\n",
    "print('字典长度为:%d'%len(idx_to_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124848, 124848, 31212, 31212)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=list(zip(all_data,all_lables))\n",
    "random.shuffle(c)\n",
    "all_data[:],all_lables[:]=zip(*c)\n",
    "length_train=int(len(all_data)*0.8)\n",
    "train_data=all_data[:length_train]\n",
    "train_lables=all_lables[:length_train]\n",
    "test_data=all_data[length_train:]\n",
    "test_lables=all_lables[length_train:]\n",
    "len(train_data),len(train_lables),len(test_data),len(test_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_train=get_tokenized_sentiment(train_data)\n",
    "ngram_test=get_tokenized_sentiment(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2idx(sentence,char_to_idx):\n",
    "    try:\n",
    "        return[char_to_idx[token]for token in sentence]\n",
    "    except (KeyError,TypeError):\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(train_data,label,batch_size):\n",
    "    data=[]\n",
    "    batch_num=len(train_data)//batch_size\n",
    "    #print(batch_num)\n",
    "    for i in range(batch_num):\n",
    "        x=train_data[max(i*batch_size,0):min((i+1)*batch_size,len(train_data))]\n",
    "        y=label[max(i*batch_size,0):min((i+1)*batch_size,len(train_data))]\n",
    "        ngram=[sentence2idx(sentence,char_to_idx) for sentence in x]\n",
    "        data.append((ngram,y))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "train_iter=dataloader(ngram_train,train_lables,batch_size)\n",
    "test_iter=dataloader(ngram_test,test_lables,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95702], [73172], [8798, 65928, 65929], [11927], [32635, 45577], [81190], [95018, 95019], [10735, 59234, 4975], [80529, 80530], [311, 50950], [68204, 90006, 90007, 90008, 24947, 24948, 1817, 90009, 90010, 53022, 2049, 10928, 90011, 90012, 90013, 90014, 90015], [96796], [4405, 56836, 56837], [18898, 6650, 5903, 64888, 64889, 9136, 64890, 64891], [7298, 8840], [2015, 27036, 27037, 27038, 27039, 27040, 5130, 27041, 27042]] [2, 2, 1, 2, 2, 2, 2, 2, 2, 3, 4, 2, 3, 1, 2, 3] 16 16\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_iter:\n",
    "    print(x,y,len(x),len(y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，数据集已经创建完毕，每次通过next(iter)调用出的数据已经是2gram词索引序列和对应的label了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步：建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature的维度为vocab_size=100664，分类的n_class=5，选择softmax regression分类器，以交叉熵损失函数作为loss，采用随机梯度下降SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def softmax(x):\n",
    "    x_exp=np.exp(x)\n",
    "    partion=np.sum(x_exp,axis=1,keepdims=True)\n",
    "    return x_exp/partion\n",
    "def feature(x):\n",
    "    batch_size=len(x)\n",
    "    feature_size=len(idx_to_char)\n",
    "    inputs=np.zeros((batch_size,feature_size))\n",
    "    #print(x)\n",
    "    #print(inputs)\n",
    "    for b,i in enumerate(x):\n",
    "        for idx in i:\n",
    "            inputs[b][idx]=1\n",
    "    return inputs\n",
    "def train(train_data,test_data,lr,num_epoch,W,b,batch_size):\n",
    "    for epoch in range(num_epoch): \n",
    "        l_sum,start,n=0.0,time.time(),0\n",
    "        train_iter=iter(train_data)\n",
    "        test_iter=iter(test_data)\n",
    "        for x,y in tqdm(train_iter):\n",
    "            x=feature(x) #[batch_size,feature]\n",
    "            probability=softmax(np.matmul(x,W)+b) #[batch_size,n_class]\n",
    "            #print(probability.shape,len(y))\n",
    "            loss= np.sum(-np.log(probability[range(probability.shape[0]), y]))\n",
    "            grad_w,grad_b=backward(x,probability,y)\n",
    "            #print(x.shape,probability.shape,loss.shape,grad_w.shape,grad_b.shape)\n",
    "            #print(grad_w,grad_b)\n",
    "            W=W-lr*grad_w\n",
    "            b=b-lr*grad_b\n",
    "            l_sum+=loss\n",
    "            n+=1\n",
    "            #print(loss)\n",
    "        print(\"epoch %d ,loss %.3f ,test_acc %.2f,time %.2f\"%(epoch+1,l_sum/n,evaluate(test_iter,W,b),time.time()-start))  \n",
    "    return W,b\n",
    "def backward(x,probability,y):\n",
    "    probability[range(probability.shape[0]), y]-=1\n",
    "    dw=x.T.dot(probability)/batch_size #feature_size*n_class\n",
    "    db=np.sum(probability,axis=0)/batch_size #n_class\n",
    "    return dw,db\n",
    "def evaluate(test_iter,W,b):\n",
    "    right=0.0\n",
    "    n=0.0\n",
    "    for x,y in test_iter:\n",
    "        n+=batch_size\n",
    "        x=feature(x)\n",
    "        probability=softmax(np.matmul(x,W)+b)\n",
    "        right+=np.sum(np.argmax(probability,axis=1)==y)\n",
    "    return right/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_iter=dataloader(ngram_train,train_lables,batch_size)\n",
    "test_iter=dataloader(ngram_test,test_lables,batch_size)\n",
    "feature_size=len(idx_to_char)\n",
    "n_class=5\n",
    "W=np.random.normal(0,0.01,(feature_size,n_class))\n",
    "b=np.zeros(n_class)\n",
    "lr,num_epoch=0.01,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 32.14it/s]\n",
      "3it [00:00, 29.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 ,loss 84.937 ,test_acc 0.51,time 68.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 32.14it/s]\n",
      "4it [00:00, 32.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 ,loss 81.675 ,test_acc 0.51,time 68.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 32.11it/s]\n",
      "4it [00:00, 32.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 ,loss 81.237 ,test_acc 0.51,time 68.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 32.23it/s]\n",
      "4it [00:00, 31.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 ,loss 80.892 ,test_acc 0.51,time 68.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 32.16it/s]\n",
      "4it [00:00, 33.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 ,loss 80.584 ,test_acc 0.51,time 68.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:01, 31.81it/s]\n",
      "4it [00:00, 30.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 ,loss 80.302 ,test_acc 0.51,time 68.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 32.16it/s]\n",
      "4it [00:00, 33.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 ,loss 80.040 ,test_acc 0.51,time 68.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 31.97it/s]\n",
      "4it [00:00, 33.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 ,loss 79.795 ,test_acc 0.51,time 68.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 32.21it/s]\n",
      "4it [00:00, 32.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 ,loss 79.563 ,test_acc 0.51,time 68.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [01:00, 32.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 ,loss 79.343 ,test_acc 0.52,time 68.10\n"
     ]
    }
   ],
   "source": [
    "train(train_iter,test_iter,lr,num_epoch,W,b,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一开始Loss有所下降，然后就几乎不变，测试集的准确率没有改变，经过一轮epoch就陷入了局部最优，也可能是模型过于简单导致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
