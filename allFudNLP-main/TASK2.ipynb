{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务二：基于深度学习的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.RNN文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from numpy import savetxt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步：数据获取与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train=pd.read_csv('E:/NLP_jupyternotebook/Fudan_NLP_beginner/data/train.tsv',header=0,delimiter='\\t')\n",
    "df_test=pd.read_csv('E:/NLP_jupyternotebook/Fudan_NLP_beginner/data/test.tsv',header=0,delimiter='\\t')\n",
    "df_train.info()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看数据信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.509945\n",
       "3    0.210989\n",
       "1    0.174760\n",
       "4    0.058990\n",
       "0    0.045316\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Sentiment.value_counts()/df_train.Sentiment.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Phrase.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理：将带有标签的数据百分之八十分为训练集，百分之二十分为测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124848, 124848, 31212, 31212)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df_train['Phrase']\n",
    "y=df_train['Sentiment']\n",
    "ceshi_data=df_test['Phrase']\n",
    "all_data=list(X)\n",
    "all_lables=list(y)\n",
    "c=list(zip(all_data,all_lables))\n",
    "random.shuffle(c)\n",
    "all_data[:],all_lables[:]=zip(*c)\n",
    "length_train=int(len(all_data)*0.8)\n",
    "train_data=all_data[:length_train]\n",
    "train_lables=all_lables[:length_train]\n",
    "test_data=all_data[length_train:]\n",
    "test_lables=all_lables[length_train:]\n",
    "len(train_data),len(train_lables),len(test_data),len(test_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentiment(data):\n",
    "    \"\"\"\n",
    "    此时的data是list of [string],进行分词\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        return[tok.lower() for tok in text.split(' ')]\n",
    "    return[tokenizer(review) for review in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('#words in vocab:', 15807)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "创建词典，并且过滤出现次数小于5的词\n",
    "\"\"\"\n",
    "def get_vocab_sentiment(data):\n",
    "    tokenized_data=get_tokenized_sentiment(data)\n",
    "    counter=collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter,min_freq=5)\n",
    "vocab=get_vocab_sentiment(all_data)\n",
    "'#words in vocab:',len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每条评论长度不一样，不能直接组合成小批量，定义一个函数将所有样本都填充或阶段成固定长度，并将词典转换成词索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentiment(data,vocab,lables=None):\n",
    "    max_l=250\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x)>max_l else x+[0]*(max_l-len(x))\n",
    "    tokenized_data=get_tokenized_sentiment(data)\n",
    "    features=torch.tensor([pad([vocab.stoi[word]for word in words])for words in tokenized_data])\n",
    "    if lables :\n",
    "        lables=torch.tensor(lables)\n",
    "        return features,lables\n",
    "    else:\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_set=Data.TensorDataset(*preprocess_sentiment(train_data,vocab,train_lables))\n",
    "test_set=Data.TensorDataset(*preprocess_sentiment(test_data,vocab,test_lables))\n",
    "train_iter=Data.DataLoader(train_set,batch_size,shuffle=True)\n",
    "test_iter=Data.DataLoader(test_set,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    7,   130,    48,    39,    32,   888,     2,   335,     3,     2,\n",
       "            18,    11,     4,    61,   125, 11052,   218,    36,   260,   338,\n",
       "             9,    56,  2679,  1808,     5,     2,  1925,  3261,     3,    43,\n",
       "            22,    12,     9,    32,  1668,    61,     8,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([256, 250]) y torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches: ', 488)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X,y in train_iter:\n",
    "    print('X',X.shape,'y',y.shape)\n",
    "    break\n",
    "'#batches: ',len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceshi_set=preprocess_sentiment(ceshi_data,vocab)\n",
    "ceshi_iter=iter(ceshi_set)\n",
    "def get_length(generator):\n",
    "    if hasattr(generator,\"__len__\"):\n",
    "        return len(generator)\n",
    "    else:\n",
    "        return sum(1 for _ in generator)\n",
    "ceshi_num=get_length(ceshi_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步：使用循环神经网络模型训练并预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型有三层，分别为：①嵌入层nn.Embeddings：将词索引转换为词向量，本实验采用两种方式进行，分别是随机初始化的词向量，且词向量可以训练；加载glove预训练好的词向量，词向量不可以训练。②编码层：nn.LSTM，选择最初实践部和最终时间步的隐藏状态连结作为下一层的输入③全连接层nn.Linear：将编码层的输出传递给全连接层进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    embed_size:词向量的维度\n",
    "    num_hiddens:隐藏状态的维度\n",
    "    num_layers：LSTM网络的深度\n",
    "    \"\"\"\n",
    "    def __init__(self,vocab,embed_size,num_hiddens,num_layers):\n",
    "        super(BiRNN,self).__init__()\n",
    "        self.embedding=nn.Embedding(len(vocab),embed_size)\n",
    "        self.encoder=nn.LSTM(input_size=embed_size,hidden_size=num_hiddens,num_layers=num_layers,bidirectional=True)\n",
    "        #双向，最终步和最初步，所以是4*num_hiddens\n",
    "        self.decoder=nn.Linear(4*num_hiddens,5) #5个分类\n",
    "    def forward(self,inputs): #inputs形状为（batch_size,词数），就是直接调用数据迭代器产生的x\n",
    "        #LSTM层输入形状必须是词数为第一个维度，所以进行转置\n",
    "        embeddings=self.embedding(inputs.permute(1,0)) #(词数，批量大小，词向量维度)\n",
    "        outputs,_=self.encoder(embeddings) #outputs,(h,c)\n",
    "        encoding=torch.cat((outputs[0],outputs[-1]),-1) #在词向量维度上进行连接\n",
    "        outs=self.decoder(encoding)\n",
    "        return outs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size,num_hiddens,num_layers=100,100,2\n",
    "net_withpretrained=BiRNN(vocab,embed_size,num_hiddens,num_layers)\n",
    "net_witoutpretrained=BiRNN(vocab,embed_size,num_hiddens,num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载预训练的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab=Vocab.GloVe(name='6B',dim=100,cache='E:/data/glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将词向量作为每个词的特征向量，先创建一个矩阵，在通过copy_()函数赋值给模型中的self.embedding\n",
    "\"\"\"\n",
    "def load_pretrain_embedding(words,pretrained_vocab): #两个都是vocab类\n",
    "    embed=torch.zeros(len(words),pretrained_vocab.vectors[0].shape[0])\n",
    "    oov_count=0\n",
    "    for i,word in enumerate(words):#词典中的词可能没有对应的词向量，所以用try except来避免异常导致的程序中断\n",
    "        try:\n",
    "            idx=pretrained_vocab.stoi[word]\n",
    "            embed[i,:]=pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count+=1\n",
    "    if oov_count>0:\n",
    "        print(\"有%d个 OOV单词.\"%oov_count)\n",
    "    return embed\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_withpretrained.embedding.weight.data.copy_(load_pretrain_embedding(vocab.itos,glove_vocab))\n",
    "net_withpretrained.embedding.weight.requires_grad=False #已经是预训练好的词向量，这里就不再需要进行更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义的模型\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter,test_iter,net,loss,optimizer,device,num_epochs):\n",
    "    net=net.to(device)\n",
    "    print(\"training on \",device)\n",
    "    batch_count=0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.cpu().sum().item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "            batch_count+=1\n",
    "        test_acc=evaluate_accuracy(test_iter,net)\n",
    "        print(\"epoch %d,loss %.4f, train acc %.3f, test acc%.3f ,time%.1f sec\"%\n",
    "              (epoch+1,train_l_sum,train_acc_sum/n,test_acc,time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对加载了预训练词向量的模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1,loss 461.8920, train acc 0.607, test acc0.639 ,time109.5 sec\n",
      "epoch 2,loss 402.8561, train acc 0.657, test acc0.661 ,time111.8 sec\n",
      "epoch 3,loss 374.2119, train acc 0.682, test acc0.666 ,time112.9 sec\n",
      "epoch 4,loss 353.4027, train acc 0.698, test acc0.672 ,time113.2 sec\n",
      "epoch 5,loss 336.5574, train acc 0.713, test acc0.670 ,time114.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs=0.01,5\n",
    "#过滤掉embedding参数，因为其不计算梯度不更新\n",
    "optimizer=torch.optim.Adam(filter(lambda p:p.requires_grad,net_withpretrained.parameters()),lr=lr)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "train(train_iter,test_iter,net_withpretrained,loss,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数一直在降低，训练集的准确率一直提升，测试集的准确率略微有下降趋向，可能出现过拟合，究其原因还是训练集太少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对没有随机初始化嵌入层的模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1,loss 769.3304, train acc 0.507, test acc0.505 ,time105.6 sec\n",
      "epoch 2,loss 769.3307, train acc 0.507, test acc0.505 ,time118.4 sec\n",
      "epoch 3,loss 769.3285, train acc 0.507, test acc0.505 ,time120.2 sec\n",
      "epoch 4,loss 769.3299, train acc 0.507, test acc0.505 ,time120.2 sec\n",
      "epoch 5,loss 769.3302, train acc 0.507, test acc0.505 ,time121.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs=0.01,5\n",
    "#过滤掉embedding参数，因为其不计算梯度不更新\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "train(train_iter,test_iter,net_witoutpretrained,loss,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机初始化的词向量训练效果很差，都远低于预训练的词向量，但是这里每轮训练的损失函数和准确率几乎不改变，这一点没有想通，可能是因为梯度消失？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trianing on cuda\n",
      "已预测5000 个样本，剩余 61292个样本，花费时间43.676 s\n",
      "已预测10000 个样本，剩余 56292个样本，花费时间43.636 s\n",
      "已预测15000 个样本，剩余 51292个样本，花费时间45.879 s\n",
      "已预测20000 个样本，剩余 46292个样本，花费时间45.666 s\n",
      "已预测25000 个样本，剩余 41292个样本，花费时间44.254 s\n",
      "已预测30000 个样本，剩余 36292个样本，花费时间44.019 s\n",
      "已预测35000 个样本，剩余 31292个样本，花费时间44.037 s\n",
      "已预测40000 个样本，剩余 26292个样本，花费时间44.281 s\n",
      "已预测45000 个样本，剩余 21292个样本，花费时间43.901 s\n",
      "已预测50000 个样本，剩余 16292个样本，花费时间45.497 s\n",
      "已预测55000 个样本，剩余 11292个样本，花费时间44.060 s\n",
      "已预测60000 个样本，剩余 6292个样本，花费时间43.879 s\n",
      "已预测65000 个样本，剩余 1292个样本，花费时间43.970 s\n"
     ]
    }
   ],
   "source": [
    "def ceshi_predict(data_iter,net,device):\n",
    "    net=net.to(device)\n",
    "    print(\"trianing on\",device)\n",
    "    predict=[]\n",
    "    i=0\n",
    "    start=time.time()\n",
    "    for X in data_iter:\n",
    "        X=X.to(device)\n",
    "        y=net(X.view(1,-1)).argmax(dim=1)\n",
    "        predict.append(y.cpu().item())\n",
    "        i+=1\n",
    "        if(i%5000==0):\n",
    "            print(\"已预测%d 个样本，剩余% d个样本，花费时间%.3f s\"%(i,(ceshi_num-i),time.time()-start))\n",
    "            start=time.time()\n",
    "    return predict\n",
    "ceshi_iter=iter(ceshi_set)\n",
    "predict_list=ceshi_predict(ceshi_iter,net_withpretrained,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[[index+156061,x]for index,x in enumerate(predict_list)]\n",
    "savetxt('E:/data/sentiment-analysis-on-movie-reviews/rnn_benchmark.csv',pred,delimiter=',',fmt='%d,%d',header='PhraseId,Sentiment',comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后提交通过加载glove词向量训练后的模型预测出的标签，kaggle得分0.649,还不错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![%E6%8D%95%E8%8E%B7.PNG](attachment:%E6%8D%95%E8%8E%B7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.使用卷积神经网络进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步：数据获取与预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步与使用循环神经网络完全相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步：使用卷积神经网络模型训练并预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 时序最大池化层(Max-over-time pooling)：提取出每个通道最大的数，用于后续在通道维进行连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d,self).__init__()\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x:(batch_size,channel,seq_len)\n",
    "        return:(batch_size,channel,1)\n",
    "        \"\"\"\n",
    "        return F.max_pool1d(x,kernel_size=x.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,vocab,embed_size,kernel_sizes,num_channels):\n",
    "        super(TextCNN,self).__init__()\n",
    "        \"\"\"依据论文，同时采用pre-train and tast-specific vectors\"\"\"\n",
    "        self.embedding=nn.Embedding(len(vocab),embed_size)\n",
    "        self.constant_embedding=nn.Embedding(len(vocab),embed_size) #不参与训练的嵌入层\n",
    "        self.dropout=nn.Dropout(0.5) #在倒数第二层使用，依据论文中dropout rate=0.5\n",
    "        self.decoder=nn.Linear(sum(num_channels),5) #把所有经过最大池化的在通道维连结，所以长度是输出通道维之和，最后五个分类\n",
    "        self.pool=GlobalMaxPool1d()\n",
    "        \"\"\"论文中的：one feature is extracted by one filter,using multiple filters(with varying windows sizes\"\"\"\n",
    "        self.convs=nn.ModuleList() \n",
    "        for c,k in zip(num_channels,kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels=2*embed_size,out_channels=c,kernel_size=k)) #*2因为要讲两个embeddings在词向量维连接起来\n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        inputs:(batch_size,seq_len)\n",
    "        embedding:(batch_size,seq_len,2*embed_size)\n",
    "        \"\"\"\n",
    "        embeddings=torch.cat((self.embedding(inputs),self.constant_embedding(inputs)),dim=2)\n",
    "        #查阅官方文档，卷积层的输入要求是（batch_size,channels,seq_len）\n",
    "        embeddings=embeddings.permute(0,2,1)\n",
    "        #对于每一个卷积层，通过时序最大池化后得到的输出是(batch_size,channels,1)，要把最后一维去掉，然后再通道维上连结，得到（batch_size,channels\n",
    "        encoding=torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1)for conv in self.convs],dim=1)\n",
    "        outputs=self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size,kernel_sizes,nums_channels=100,[3,4,5],[100,100,100] #完全依据原论文设置\n",
    "net=TextCNN(vocab,embed_size,kernel_sizes,nums_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载预训练词向量：两个嵌入层都加载glove词向量，然后把一个嵌入层权重固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有1157个 OOV单词.\n",
      "有1157个 OOV单词.\n"
     ]
    }
   ],
   "source": [
    "glove_vocab=Vocab.GloVe(name='6B',dim=100,cache='E:/data/glove')\n",
    "net.embedding.weight.data.copy_(load_pretrain_embedding(vocab.itos,glove_vocab))\n",
    "net.constant_embedding.weight.data.copy_(load_pretrain_embedding(vocab.itos,glove_vocab))\n",
    "net.constant_embedding.weight.require_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练并评价模型：使用与RNN相同的训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1,loss 461.5254, train acc 0.629, test acc0.645 ,time70.0 sec\n",
      "epoch 2,loss 435.8141, train acc 0.649, test acc0.642 ,time76.3 sec\n",
      "epoch 3,loss 426.0263, train acc 0.659, test acc0.642 ,time75.5 sec\n",
      "epoch 4,loss 426.5640, train acc 0.662, test acc0.644 ,time76.8 sec\n",
      "epoch 5,loss 425.5484, train acc 0.667, test acc0.646 ,time74.3 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epochs=0.01,5\n",
    "optimizer=torch.optim.Adam(filter(lambda p:p.requires_grad,net.parameters()),lr=lr)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "train(train_iter,test_iter,net,loss,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练效果较差，准确率没有明显提升，损失函数也并非一直下降，推测是学习率过高的原因，测试lr=0.001效果好很多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试集输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trianing on cuda\n",
      "已预测5000 个样本，剩余 61292个样本，花费时间14.821 s\n",
      "已预测10000 个样本，剩余 56292个样本，花费时间14.027 s\n",
      "已预测15000 个样本，剩余 51292个样本，花费时间14.517 s\n",
      "已预测20000 个样本，剩余 46292个样本，花费时间14.458 s\n",
      "已预测25000 个样本，剩余 41292个样本，花费时间15.408 s\n",
      "已预测30000 个样本，剩余 36292个样本，花费时间15.609 s\n",
      "已预测35000 个样本，剩余 31292个样本，花费时间14.713 s\n",
      "已预测40000 个样本，剩余 26292个样本，花费时间14.976 s\n",
      "已预测45000 个样本，剩余 21292个样本，花费时间14.020 s\n",
      "已预测50000 个样本，剩余 16292个样本，花费时间13.123 s\n",
      "已预测55000 个样本，剩余 11292个样本，花费时间13.278 s\n",
      "已预测60000 个样本，剩余 6292个样本，花费时间12.435 s\n",
      "已预测65000 个样本，剩余 1292个样本，花费时间13.083 s\n"
     ]
    }
   ],
   "source": [
    "def ceshi_predict(data_iter,net,device):\n",
    "    net=net.to(device)\n",
    "    print(\"trianing on\",device)\n",
    "    predict=[]\n",
    "    i=0\n",
    "    start=time.time()\n",
    "    for X in data_iter:\n",
    "        X=X.to(device)\n",
    "        y=net(X.view(1,-1)).argmax(dim=1)\n",
    "        predict.append(y.cpu().item())\n",
    "        i+=1\n",
    "        if(i%5000==0):\n",
    "            print(\"已预测%d 个样本，剩余% d个样本，花费时间%.3f s\"%(i,(ceshi_num-i),time.time()-start))\n",
    "            start=time.time()\n",
    "    return predict\n",
    "ceshi_iter=iter(ceshi_set)\n",
    "predict_list=ceshi_predict(ceshi_iter,net,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[[index+156061,x]for index,x in enumerate(predict_list)]\n",
    "savetxt('E:/data/sentiment-analysis-on-movie-reviews/cnn_benchmark.csv',pred,delimiter=',',fmt='%d,%d',header='PhraseId,Sentiment',comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明显看出来结果不如RNN好，kaggle提交得分也是，但是训练时间和测试时间确实少了不少，可能是添加了dropout层的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![%E6%8D%95%E8%8E%B7.PNG](attachment:%E6%8D%95%E8%8E%B7.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
